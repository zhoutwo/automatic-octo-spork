\def\year{2018}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title (Sentence Summarization Using a Worker-Supervisor Approach)
/Author (Zhou Zhou)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Sentence Summarization Using a Worker-Supervisor Approach}
\author{Zhou Zhou\\
Rose-Hulman Institute of Technology\\
5500 Wabash Avenue\\
Terre Haute, IN 47803\\
}
\maketitle
\begin{abstract}
AAAI creates proceedings, working notes, and technical reports directly from electronic source furnished by the authors. To ensure that all papers in the publication have a uniform appearance, authors must adhere to the following instructions. 
\end{abstract}

\section{Introduction}
In recent years, the field of language modeling has seen dramatic improvements in many aspects, especially in terms of machine translation models. Recurrent Neural Networks, Long Short-Term Memory, attention module, and the encoder-decoder structure, all are part of the reasons machine translation models performing phenomenally better than before.

However, it’s not just the machine translation tasks get performed better. Language modeling is essential to many Natural Language Processing tasks, one of which is sentence summarization. As there are more tools in the toolbox, significant innovations and improvements are observed in this field as well.

\section{Related Work}
The work of this research stems directly from that of Facebook researchers have done. Their first system achieved their goal using an attention-based encoder, a beam search based decoder, and a generation algorithm \cite{rush2015neural}. Later on, these researchers at Facebook worked further on improving the model. First, they improved the way of training the model. Instead of training the model with ground truth data, they used generated data to train the rest of the network \cite{ranzato2015sequence}. On the other hand, a group of joint researchers from Tsinghua University and University of Toronto employed a “Read-Again” strategy, scanning the input two times to point the attention better. They also developed a copy mechanism to copy words from input to output, so  rare words can be simply copied over, resulting in a smaller vocabulary size \cite{zeng2016efficient}. Although their groups have different research directions, both groups pointed out that their work has been based on the work of neural machine translation methods, mostly from Bengio et al \cite{bahdanau2014neural}. This shows the connection between the sentence summarization task and the machine translation task. In fact, one can think of sentence summarization as a special form of machine translation, where the source language and the target language are the same. This makes it important to also consider the recent developments in the field of machine translation using neural network as well.

Machine translation has been seen as a hard problem because word dependencies can be long with huge chunks of texts, and models are either stable against noise or efficiently trainable, but not both \cite{bengio1994learning}. After the invention of Long Short-Term Memory, this problem was mediated because LSTM learns to map an input sentence of variable length into fixed-dimensional vector representation \cite{sutskever2014sequence}. This allowed further innovations to follow up. Bengio et al. created a successful system using bidirectional RNN as the encoder of the input sentence \cite{bahdanau2014neural}. However, a great breakthrough was made by Google Brain researchers \cite{johnson2016google}. They used a deep LSTM network with 8 layers as the encoder and another 8 layers as the decoder. Moreover, they used residual connections in the encoder and an attention mechanism in the decoder. This created the phenomenon of “Zero-Shot Translation”, where the model can learn to translate between language pair A and C given only data on language pair A and B and language pair B and C. another way to obtain a system that can do zero-shot translation is to use a pivot language as the intermediate language B at training time, where the model is trained on language pair A and B, and B and C, which has also been shown to be effective \cite{chen2017teacher}. Some researchers also attempted to tackle the drawbacks of an artificial evaluation method. Yang, Zhen, et al. built a generative adversarial network where the discriminator grades the output instead of using an artificial evaluation function (such as BLEU) \cite{yang2017improving}. Given that sequence-to-sequence models tend to be very large, attempts were also made to reduce the training time of a large network, such as the Mixture-of-Experts layer that avoids training unused portion of the network given a specific input \cite{shazeer2017outrageously}. Lastly, to overcome the issues of the model forgetting the rare words that have only a few occurrences, Feng, Yang, et al. augmented their model with a memory that maps source words that have rare occurring frequencies to their translations \cite{feng2017memory}.

\section{Model Architecture}
We combine the idea of the encoder-decoder architecture with a custom Read-Again strategy, and propose the worker-supervisor architecture. In this architecture, the worker produces the summarized text using the given input. This initial output is concatenated with the original input, and the combined text is then fed into the supervisor to determine whether the summary is acceptable. If the supervisor decides against this summary, the summary is then fed back into the worker so the worker can read both the original input and its own original output, and come up with a new summary, hopefully of better quality than the original. This process may repeat for a given number of times. We will describe the architecture of this architecture in details below.

\subsection{Worker}
A worker should be itself a full sequence-to-sequence model that can generate summary with or without a supervisor. As any existing text summarization model should be able to take this role, we picked Google’s GNMT model as the worker [5]. GNMT uses 8 LSTM layers with 1024 units on either side, with the encoder using residual connections and the encoder using the attention mechanism. These features have allowed GNMT to perform well in translation tasks, even between multiple language pairs. This capacity should be more than enough to take on the summarization task, which is between one language pair in which both the source language and the target language are the same.

\subsection{Supervisor}
A supervisor should also be an RNN to be able to read a sequence with arbitrary length. It can be simpler than the worker though, as it does not need to generate text outputs. Instead, it should generate a score to describe how much it thinks a concatenated text contains an acceptable summary. We used 8 normally-stacked LSTM layers with 1024 units and a fully connected layer connected to the last layer. The fully connected layer has only one output, and uses the sigmoid function as its activation function to guarantee an output between 0 and 1.

\subsection{Embedding}
Embedding layers are used to convert each text token (in the form of an integer value) into a vector of some representation []. This process takes place before the input is fed into the encoder, the decoder, and the supervisor. A well trained embedding would generate the representation based on the meaning of the word, so sometimes it would place synonyms in spatially close locations. In a machine translation task, separate embeddings are used by the encoder and the decoder, so one may naturally question, since we are using the same source and target language, do we need two embeddings or one? We decided to go with two embeddings, because the task of the decoder is significantly different from the encoder, and it might need different information from its inputs.

\subsection{Training the Model}
We train the model with the following procedure:
\begin{itemize}
	\item The worker is trained with a batch of sentence pairs from the training set.
	\item The supervisor is trained with a batch consisting of concatenated input and output sentence and a score of 0.99. We are not using 1 because using extreme values (0 and 1) during training time is not preferred in GAN training [].
	\item The worker is run to get a batch of inferred sentences.
	\item These sentences are concatenated with their source sentences, then sent to the supervisor with a score of 0.01.
	\item These sentences are paired up with the actual target sentences, and then sent to the worker to train it.
\end{itemize}

\subsection{Running the Model}
The way to run the model is different than training because we allow the worker to retry the work multiple times, and a threshold score is used to determine whether the summarization needs to be redone.
\begin{itemize}
	\item The worker runs on the input sentence to generate an initial summary.
	\item The summary and the input sentence are concatenated and sent to the supervisor to obtain a score.
	\item If a score is greater than a specified threshold, then output the summary.
	\item Otherwise, the concatenated sentence is sent back to worker and repeat the steps above. These steps will only execute for at most a specified number of times.
\end{itemize}

\section{Environment Setup}

\bibliography{References}
\bibliographystyle{aaai}
\end{document}
